{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Droping Columns\n",
    "- Droping Rows\n",
    "- Various parameter in droping Functionality \n",
    "- Handling Missing Value By Mean, Median, Moad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('parctice2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Expirience Salary: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('missdata.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('missdata.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+\n",
      "|    Name| Age|Expirience Salary|\n",
      "+--------+----+-----------------+\n",
      "| Avinash|  28|           100000|\n",
      "|Saipriya|  26|            90000|\n",
      "|   Anuja|  26|            80000|\n",
      "| Shubhee|  22|            50000|\n",
      "|  Surbhi|  23|            33000|\n",
      "|   Pooja|null|             null|\n",
      "|Priyanka|  29|            61000|\n",
      "|Harshda |  30|            83000|\n",
      "|    null|  34|            71000|\n",
      "|    null|  31|             null|\n",
      "+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('missdata.csv',header=True,inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| Age|Expirience Salary|\n",
      "+----+-----------------+\n",
      "|  28|           100000|\n",
      "|  26|            90000|\n",
      "|  26|            80000|\n",
      "|  22|            50000|\n",
      "|  23|            33000|\n",
      "|null|             null|\n",
      "|  29|            61000|\n",
      "|  30|            83000|\n",
      "|  34|            71000|\n",
      "|  31|             null|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+\n",
      "|    Name| Age|Expirience Salary|\n",
      "+--------+----+-----------------+\n",
      "| Avinash|  28|           100000|\n",
      "|Saipriya|  26|            90000|\n",
      "|   Anuja|  26|            80000|\n",
      "| Shubhee|  22|            50000|\n",
      "|  Surbhi|  23|            33000|\n",
      "|   Pooja|null|             null|\n",
      "|Priyanka|  29|            61000|\n",
      "|Harshda |  30|            83000|\n",
      "|    null|  34|            71000|\n",
      "|    null|  31|             null|\n",
      "+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+\n",
      "|    Name|Age|Expirience Salary|\n",
      "+--------+---+-----------------+\n",
      "| Avinash| 28|           100000|\n",
      "|Saipriya| 26|            90000|\n",
      "|   Anuja| 26|            80000|\n",
      "| Shubhee| 22|            50000|\n",
      "|  Surbhi| 23|            33000|\n",
      "|Priyanka| 29|            61000|\n",
      "|Harshda | 30|            83000|\n",
      "+--------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping null values\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+\n",
      "|    Name| Age|Expirience Salary|\n",
      "+--------+----+-----------------+\n",
      "| Avinash|  28|           100000|\n",
      "|Saipriya|  26|            90000|\n",
      "|   Anuja|  26|            80000|\n",
      "| Shubhee|  22|            50000|\n",
      "|  Surbhi|  23|            33000|\n",
      "|   Pooja|null|             null|\n",
      "|Priyanka|  29|            61000|\n",
      "|Harshda |  30|            83000|\n",
      "|    null|  34|            71000|\n",
      "|    null|  31|             null|\n",
      "+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any==how\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+\n",
      "|    Name|Age|Expirience Salary|\n",
      "+--------+---+-----------------+\n",
      "| Avinash| 28|           100000|\n",
      "|Saipriya| 26|            90000|\n",
      "|   Anuja| 26|            80000|\n",
      "| Shubhee| 22|            50000|\n",
      "|  Surbhi| 23|            33000|\n",
      "|Priyanka| 29|            61000|\n",
      "|Harshda | 30|            83000|\n",
      "+--------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+\n",
      "|    Name|Age|Expirience Salary|\n",
      "+--------+---+-----------------+\n",
      "| Avinash| 28|           100000|\n",
      "|Saipriya| 26|            90000|\n",
      "|   Anuja| 26|            80000|\n",
      "| Shubhee| 22|            50000|\n",
      "|  Surbhi| 23|            33000|\n",
      "|Priyanka| 29|            61000|\n",
      "|Harshda | 30|            83000|\n",
      "|    null| 34|            71000|\n",
      "+--------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshhold   # threshhold if we put thresh=2 means atlest 2 null value must be present in any row\n",
    "df_pyspark.na.drop(how=\"any\",thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+\n",
      "|    Name|Age|Expirience Salary|\n",
      "+--------+---+-----------------+\n",
      "| Avinash| 28|           100000|\n",
      "|Saipriya| 26|            90000|\n",
      "|   Anuja| 26|            80000|\n",
      "| Shubhee| 22|            50000|\n",
      "|  Surbhi| 23|            33000|\n",
      "|Priyanka| 29|            61000|\n",
      "|Harshda | 30|            83000|\n",
      "+--------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+\n",
      "|    Name|Age|Expirience Salary|\n",
      "+--------+---+-----------------+\n",
      "| Avinash| 28|           100000|\n",
      "|Saipriya| 26|            90000|\n",
      "|   Anuja| 26|            80000|\n",
      "| Shubhee| 22|            50000|\n",
      "|  Surbhi| 23|            33000|\n",
      "|Priyanka| 29|            61000|\n",
      "|Harshda | 30|            83000|\n",
      "|    null| 34|            71000|\n",
      "+--------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset # if we want to drop a null value any specific columns\n",
    "df_pyspark.na.drop(how='any',subset=['Expirience Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------------+\n",
      "|           Name| Age|Expirience Salary|\n",
      "+---------------+----+-----------------+\n",
      "|        Avinash|  28|           100000|\n",
      "|       Saipriya|  26|            90000|\n",
      "|          Anuja|  26|            80000|\n",
      "|        Shubhee|  22|            50000|\n",
      "|         Surbhi|  23|            33000|\n",
      "|          Pooja|null|             null|\n",
      "|       Priyanka|  29|            61000|\n",
      "|       Harshda |  30|            83000|\n",
      "|Missing Value..|  34|            71000|\n",
      "|Missing Value..|  31|             null|\n",
      "+---------------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling the missing value\n",
    "df_pyspark.na.fill(\"Missing Value..\").show()   # replace missing value with null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+\n",
      "|    Name| Age|Expirience Salary|\n",
      "+--------+----+-----------------+\n",
      "| Avinash|  28|           100000|\n",
      "|Saipriya|  26|            90000|\n",
      "|   Anuja|  26|            80000|\n",
      "| Shubhee|  22|            50000|\n",
      "|  Surbhi|  23|            33000|\n",
      "|   Pooja|null|             null|\n",
      "|Priyanka|  29|            61000|\n",
      "|Harshda |  30|            83000|\n",
      "|    null|  34|            71000|\n",
      "|    null|  31|             null|\n",
      "+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(\"Missing Value..\",\"Expirience Salary\").show()  # replace any specific column missing value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Expirience Salary: int]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+\n",
      "|    Name| Age|Expirience Salary|\n",
      "+--------+----+-----------------+\n",
      "| Avinash|  28|           100000|\n",
      "|Saipriya|  26|            90000|\n",
      "|   Anuja|  26|            80000|\n",
      "| Shubhee|  22|            50000|\n",
      "|  Surbhi|  23|            33000|\n",
      "|   Pooja|null|             null|\n",
      "|Priyanka|  29|            61000|\n",
      "|Harshda |  30|            83000|\n",
      "|    null|  34|            71000|\n",
      "|    null|  31|             null|\n",
      "+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"inputCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36mtoString\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not convert %s to string type\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-3ed6a5cfd12e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m imputer = Imputer(\n\u001b[0;32m      4\u001b[0m     \u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Expirience Salary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0moutputCols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"{}_imputed\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Expirience Salary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     ).setStrategy(\"mean\")\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, strategy, missingValue, inputCols, outputCols, inputCol, outputCol, relativeError)\u001b[0m\n\u001b[0;32m   1659\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.ml.feature.Imputer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1661\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36msetParams\u001b[1;34m(self, strategy, missingValue, inputCols, outputCols, inputCol, outputCol, relativeError)\u001b[0m\n\u001b[0;32m   1671\u001b[0m         \"\"\"\n\u001b[0;32m   1672\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1673\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2.2.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid param value given for param \"inputCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Name','Age','Expirience Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Name','Age','Expirience Salary']]                    \n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-6b99403f857e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrasform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'imputer' is not defined"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).trasform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
